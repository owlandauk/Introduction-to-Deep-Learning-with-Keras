{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b1b87f5",
   "metadata": {},
   "source": [
    "# LSTM Text Prediction\n",
    "\n",
    "Build a text predictor using Embedding and LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a6616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample text data\n",
    "texts = [\n",
    "    \"Deep learning is fun\",\n",
    "    \"Keras makes it easy\",\n",
    "    \"We love neural networks\",\n",
    "    \"Text prediction with LSTM\"\n",
    "]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Vocabulary size:\", len(word_index))\n",
    "\n",
    "# Prepare data\n",
    "maxlen = 3\n",
    "data = pad_sequences(sequences, maxlen=maxlen, padding='pre')\n",
    "labels = np.array([1, 1, 1, 1])  # dummy\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word_index)+1, output_dim=8, input_length=maxlen),\n",
    "    LSTM(16),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train\n",
    "model.fit(data, labels, epochs=5)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
